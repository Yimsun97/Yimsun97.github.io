<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[神经网络Backpropagation算法入门]]></title>
    <url>%2F2019%2F11%2F24%2F%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9CBackpropagation%E7%AE%97%E6%B3%95%E5%85%A5%E9%97%A8%2F</url>
    <content type="text"><![CDATA[近期学习了 coursera 上 Andrew Ng 的机器学习课程[1]。总体而言，该课程是入门机器学习的绝佳教程，整体难度不大，但概括了机器学习中很多基础的概念。但弊端在于跳过了过多数学推导，导致有些问题讲解得不是很清楚。特别是在第5周课程 Neural Networks: Learning 中，虽讲解了 BP 算法的轮廓和执行过程，但没有计算图的辅助和数学公式的推导就很难理解，且出现了较多的讹误。因此写下该篇笔记，记录关于BP算法的重要学习路线和结论，以期给其他同样困惑的同学和未来的自己一些提示作用。 该笔记的写作过程中，同样参考了斋藤康毅的《深度学习入门》一书中计算图（computational graph）的叙述，和长躯鬼侠的知乎专栏矩阵求导术（上）中关于标量对矩阵导数计算的说明。 概述BP 神经网络的实现过程大体上如下： Step 1 随机产生网络参数的初始值， Step 2 计算正向传播（Forward propagation）得到预测值，进而计算损失函数的值； Step 3 通过 Backpropagation 算法计算偏导数，进而得到损失函数的梯度； Step 4 利用优化算法对参数值进行更新； Step 5 反复执行Step 2 ~ Step 4 至收敛为止。 Backpropagation 算法是计算神经网络梯度（Step 3）的重要算法，比起计算导数的差分算法，它大大提升了导数的计算速度，进而加快了神经网络的训练速度。 符号系统本篇笔记的符号系统与 coursera 课程上的基本保持一致，略有修改。简述如下： 图1为神经网络的示例图（未画出偏置项），该网络用于一个多元分类问题，共分为 $K$ 类。 图1 神经网络示例用 $L$ 表示神经网络总层数， $s_l$ 表示第 $l$ 层的神经元个数（不包括偏置项），共有 $M$ 个数据，数据记为 {\left\{(x^{(1)},y^{(1)}),(x^{(2)},y^{(2)}),\cdots,(x^{(M)},y^{(M)}) \right\}}激活函数记为 h_\theta(x)\in \mathbb{R}^K$(h_\theta(x))_i$ 表示第 $i$ 个结果。 损失函数记为 $J(\Theta)$ ,表达式如下： \begin{aligned} J(\Theta)=&-\frac{1}{M}\left[\sum_{m=1}^{M} \sum_{k=1}^{K} y_{k}^{(m)} \log \left(h_{\Theta}\left(x^{(m)}\right)\right)_{k}+\left(1-y_{k}^{(m)}\right) \log \left(1-\left(h_{\Theta}\left(x^{(m)}\right)\right)_{k}\right)\right] \\ &+\frac{\lambda}{2 M} \sum_{l=1}^{L-1} \sum_{i=1}^{s_{l}} \sum_{j=1}^{s_{l+1}}\left(\Theta_{j i}^{(l)}\right)^{2} \end{aligned}式中， $M$ 为样本个数， $K$ 为分类总数， $\Theta_{ji}^{(l)}$ 为第 $l$ 层后节点编号为 $j$ 、前节点编号为 $i$ 的参数值。 记激活函数 sigmoid 函数为 $g(x)$ ，即： g(x)=\frac{1}{1+e^{-x}}记 $\delta_j^{(l)}$ 为第 $l$ 层，第 $j$ 个节点的”误差”（coursera: “error”），下面的叙述会具体解释该参数的含义。 前向传播以 one-hot 表示的单一数据 $(x,y)$ 为例，正向传播过程的表示如下： \begin{array}{l}{a^{(1)}=x\left(\operatorname{add} a_{0}^{(1)}\right)} \\ {z^{(2)}=\Theta^{(1)} a^{(1)}} \\ {a^{(2)}=g\left(z^{(2)}\right)\left(\operatorname{add} a_{0}^{(2)}\right)} \\ {z^{(3)}=\Theta^{(2)} a^{(2)}} \\ {a^{(3)}=g\left(z^{(3)}\right)\left(\operatorname{add} a_{0}^{(3)}\right)} \\ {z^{(4)}=\Theta^{(3)} a^{(3)}} \\ {a^{(4)}=h_{\Theta}(x)=g\left(z^{(4)}\right)}\end{array}本文重点是反向传播，前向传播推导从略。 反向传播先给出算法，后进行解释，最后进行推导。 算法描述coursera 课程上的算法描述如下： Step 1 Training set ${\left\{(x^{(1)},y^{(1)}),(x^{(2)},y^{(2)}),\cdots,(x^{(M)},y^{(M)}) \right\}}$ Step 2 Set $\Delta_{ji}^{(l)}=0$ (for all $l,i,j$). Step 3 For $m=1,M$ ​ Set $a^{(1)}=x^{(m)}$ ​ Perform forward propagation to compute $a^{(l)}$ for $l=2,3,\cdots,L$ ​ Compute $\delta^{(L)},\delta^{(L-1)},\delta^{(L-2)},\cdots,\delta^{(2)}$ ​ $\Delta_{ji}^{(l)}:=\Delta_{ji}^{(l)}+a_j^{(l)}\delta_{i}^{(l+1)}$ Step 4 Computer $D_{ji}^{(l)}$: ​ $D_{ji}^{(l)}=\frac{1}{M}(\Delta_{ji}^{(l)}+\lambda\Theta_{ji}^{(l)})$ if $i\neq0$ ​ $D_{ji}^{(l)}=\frac{1}{M}\Delta_{ji}^{(l)}$ if $i=0$ 其中 $\delta^{(l)}$ 的计算如下： \delta^{(4)}=a^{(4)}-y\\ \delta^{(3)}=\left(\Theta^{(3)}\right)^{T} \delta^{(4)} \cdot * g^{\prime}\left(z^{(3)}\right)\\ \delta^{(2)}=\left(\Theta^{(2)}\right)^{T} \delta^{(3)} \cdot * g^{\prime}\left(z^{(2)}\right)计算图及公式推导 要正确理解 BP 算法，有两种方法，一种是基于数学式，一种是基于计算图。前者是比较常见的方法，该方法严密简洁且合理，但如果一上来就围绕数学式进行探讨，会忽略一些根本的东西[2]。但笔者认为，只根据计算图进行理解，不利于后续的编程计算。因而本文将二者相结合进行算法的推导。 计算图按照图中的数据类型可分为两类：标量计算图和矩阵计算图。 标量计算图对于标量计算图，根据链式法则可以得到下图所示的结果： 图2 标量计算图在该图中，为使表示清晰，我们省略了反向传播的箭头。以箭头上方的数字表示正向传播的数值，箭头下方的为反向传播的数值。 矢量计算图1对于矢量计算图，涉及到标量对矩阵的导数[3]，输出层的计算图如图3。 图3 矩阵计算图1计算推导过程如下： 若不考虑正则化，样本数量为 $M$ 时的总损失函数的矢量化形式为 J'=-y^Tlog(a^{(4)})+(\mathbf{1}-y)^Tlog(\mathbf{1}-a^{(4)})其中， $\mathbf{1}$ 为元素全为1的与 $y$ 尺寸相同的矩阵。这里的总损失函数 $J’=J\cdot M$ 。 \mathrm{d}J'=-tr\left[y^T\frac{\mathrm{d}a^{(4)}}{a^{(4)}}-(\mathbf{1}-y)^T\frac{\mathrm{d}a^{(4)}}{a^{(4)}}\right]=tr\left\{\left[(\frac{y}{a^{(4)}})^T-(\frac{\mathbf{1}-y}{\mathbf{1}-a^{(4)}})^T\right]\mathrm{d}a^{(4)}\right\}故有 \frac{\partial J’}{\partial a^{(4)}}=\frac{\mathbf{1}-y}{\mathbf{1}-a^{(4)}}-\frac{y}{a^{(4)}}进而 \delta^{(4)}=\frac{\partial J’}{\partial z^{(4)}}=\frac{\partial J’}{\partial a^{(4)}}\frac{\partial a^{(4)}}{\partial z^{(4)}}=\left[\frac{\mathbf{1}-y}{\mathbf{1}-a^{(4)}}-\frac{y}{a^{(4)}}\right]\odot a^{(4)}\odot (1-a^{(4)})=a^{(4)}-y式中，符号 $\odot$ 为逐元素乘法。 由此，得到如图3所示的计算图。 矢量计算图2按照矢量计算图1的思路，继续反向传播，以正向传播中的 ${z^{(4)}=\Theta^{(3)} a^{(3)}}$ 为例，我们给出如下计算图： 图4 矩阵计算图2计算推导过程如下： \mathrm{d}J'=tr(\frac{\partial J'}{\partial z^{(4)}}^T\mathrm{d}z^{(4)})=tr(\frac{\partial J'}{\partial z^{(4)}}^T\mathrm{d}\Theta^{(3)}\cdot a^{(3)})=tr(a^{(3)}\frac{\partial J'}{\partial z^{(4)}}^T\mathrm{d}\Theta^{(3)})故有 \frac{\partial J'}{\partial \Theta^{(3)}}=\frac{\partial J'}{\partial z^{(4)}}(a^{(3)})^T=\delta^{(4)}(a^{(3)})^T类似地，有 \frac{\partial J'}{\partial a^{(3)}}=(\Theta^{(3)})^T\frac{\partial J'}{\partial z^{(4)}}=(\Theta^{(3)})^T\delta^{(4)}进而 \delta^{(3)}=\frac{\partial J’}{\partial z^{(3)}}=\frac{\partial J'}{\partial a^{(3)}}\frac{\partial a^{(3)}}{\partial z^{(3)}}=(\Theta^{(3)})^T\delta^{(4)}\odot a^{(3)}\odot (1-a^{(3)})以此类推，有 \delta^{(2)}=(\Theta^{(2)})^T\delta^{(3)}\odot a^{(2)}\odot (1-a^{(2)})从以上的推导中可以发现， $\delta^{(l)}$ 的定义为总损失函数对 $z^{(l)}$ 的偏导数，即 \delta^{(l)}=\frac{\partial J'}{\partial z^{(l)}}由此自然可以得到 \Delta:=\Delta+\delta^{(l+1)}(a^{(l)})^T这就是算法描述中的 $\Delta_{ji}^{(l)}$ 表达的矢量化表达。 若考虑正则化和损失函数的平均值 $J=J’/M$ ，自然有算法描述中的 D_{ji}^{(l)}=\frac{1}{M}(\Delta_{ji}^{(l)}+\lambda\Theta_{ji}^{(l)}),i\neq0\\ D_{ji}^{(l)}=\frac{1}{M}\Delta_{ji}^{(l)},i=0需要注意的是，由于前向传播时，每到下一层节点时，总是先添加偏置项（$a_0^{(1)},a_0^{(2)},a_0^{(3)}$）再正向传播计算对应的（$z^{(2)},z^{(3)},z^{(4)}$），所以在反向传播时得到（$\delta^{(3)},\delta^{(2)}$）时应去掉第一个元素再进行反向传播。一般地，应去调（$\delta^{(l-1)},\delta^{(l-2)},\cdots,\delta^{(l-1)}$）中的第一个元素再进行反向传播。这一做法按照计算图是十分易于理解的。 经过以上推导，我们对 BP 算法的反向传播过程有了更加深入的了解，也加深了对算法描述中的内容的理解。 注解 斋藤康毅《深度学习入门》一书中，输出层的激活函数为 softmax ,而损失函数采用了交叉熵误差函数，可以证明，这两个函数得到的效果与 Coursera 中输出层采用 sigmoid 函数和相应损失函数的效果是一致的，都是使 $\delta^{(4)}=a^{(4)}-y$ 成立，也就是将”误差“返回给上一层（过程见矩阵求导术（上）例6解2）。这样的”漂亮“结果是针对问题特意设计得到的。 关于算法描述的证明，可以用标量的形式，与矩阵形式的证明同理，过程稍显繁琐，具体过程可参考斋藤康毅《深度学习入门》一书第 5 章及附录。 参考文献[1] Andrew Ng 的 Coursera 课程 Machine Learning [2] 斋藤康毅.深度学习入门:基于Python的理论与实现.北京:人民邮电出版社,2018.121~161 [3] 长驱鬼侠的知乎专栏.矩阵求导术（上）]]></content>
      <categories>
        <category>数学</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Beamer制作幻灯片时分文件编译]]></title>
    <url>%2F2019%2F10%2F05%2FBeamer%E5%88%B6%E4%BD%9C%E5%B9%BB%E7%81%AF%E7%89%87%E6%97%B6%E5%88%86%E6%96%87%E4%BB%B6%E7%BC%96%E8%AF%91%2F</url>
    <content type="text"><![CDATA[在用 Beamer 制作幻灯片的过程中，如果页数或者图片过多，往往造成 LaTeX 编译速度慢的问题。此时，如果能够分文件进行编译查看效果，最后插入到主文件中编译，制作幻灯片的效率势必大大提高。本文主要介绍Beamer制作幻灯片时分文件编译的方法。 Beamer 简介Beamer 是一个用于创建演示文稿 LaTeX 的文档类。Beamer 创建演示文稿相对于 PowerPoint 来说，优点在于简介大方，同时也有很多可直接运用的美观的模板，适合于数学公式较多的学术演示，移植性较好；缺点是不如 PowerPoint 那样直观灵活，动画支持较差，多图片排版较繁琐，播放时需要如 Adobe Reader 之类的阅读器。 网络上的Beamer教程较多，本文不再赘述其基本用法。更多关于 Beamer 作展示的利弊讨论参考 Why should I use LaTeX for presentations? 总之， Beamer 较适用于数学公式较多、图片较少，注重内容的学术展示。 关于可用的 Beamer 版式主题和颜色主题，这个 网站 给出了二者的组合，可以在比较后选择喜欢的用 \usetheme{} 引用版式主题，用 usecolortheme{} 引用颜色主题。 Beamer 分文件编译这里我们介绍利用 Beamer 分文件编译的一种方法。参考了 StackExchange 上的 这篇帖子 。基本思路是利用 Tomasz M. Trzeciak 编写的 docmute 包，该包文档参考 The docmute package 。 我的环境配置是 TeX Live 2017 + TeX Studio 2018 。 首先，我们打开 TeX Studio 新建一个主文件 main.tex ，内容如下： 12345678910111213141516\documentclass&#123;ctexbeamer&#125;\usepackage&#123;filecontents&#125;\begin&#123;filecontents*&#125;&#123;main.tex,pg1.tex&#125;\documentclass&#123;beamer&#125;\begin&#123;document&#125;\begin&#123;frame&#125; 世界！\end&#123;frame&#125;\end&#123;document&#125;\end&#123;filecontents*&#125;\usepackage&#123;docmute&#125;\begin&#123;document&#125;\input&#123;main&#125;\input&#123;pg1&#125;\end&#123;document&#125; 然后，我们新建一个名为 pg1 的子文件： 123456\documentclass&#123;ctexbeamer&#125;\begin&#123;document&#125;\begin&#123;frame&#125; 你好！\end&#123;frame&#125;\end&#123;document&#125; 此时，主文件和子文件均可独立编译。在 TeX Studio 中的具体方法是 主文件：直接编译即可； 子文件：先将子文件设置为主文档，再编译就是只编译子文件得到的效果。设置主文档方法如下图。 编译主文件得到效果如下： 只编译子文件得到效果如下： 由此，我们可以将 Beamer 分为若干个 Section （一般为3~5个比较好），分别制作每个部分的子文后编译查看效果，若内容和排版均符合要求，即可编译主文件。这样一方面节省了编译时间，另一方面也提高了代码的可读性，提高了工作效率。 参考资料 Beamer 版式和颜色主题组合 Beamer 的应用场景 Beamer 分文件编译的方法 docmute 包源码文档]]></content>
      <categories>
        <category>程序语言</category>
      </categories>
      <tags>
        <tag>LaTeX</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[win10上GPU版PyTorch的环境配置]]></title>
    <url>%2F2019%2F09%2F25%2Fwin10%E4%B8%8AGPU%E7%89%88PyTorch%E7%9A%84%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[本文主要解决Win10系统上GPU版PyTorch的环境配置问题。 检查配置按 Win+X 键打开设备管理器，找到显示适配器，查看自己电脑的显卡配置，如下图所示。 我的电脑是NVIDIA GeForce GTX 960M的显卡。 更新驱动打开NVIDIA官网更新驱动，选择第一步检查的自己电脑的配置。以我的电脑为例，依次为选择“GeForce”，“GeForce 900M Series (Notebooks)”，”GeForce GTX 960M”，“Windows 10 64-bit”，其他不做改动。点击“Search”后，点击“DOWNLOAD”进行下载。 下载后进行安装，安装过程中安装路径可自定义，选择“只安装图形驱动程序”即可。此过程从略。 用conda安装利用conda安装GPU版PyTorch的好处是，利用Anaconda包管理器安装cudatoolkit，避免了分别安装CUDA和cuDNN的不便。Anaconda包管理器的安装及基本用法可参见其他教程，此处从略。 打开Anaconda Promt，执行conda create -n pt1 python=3.6 spyder，创建环境pt1。 打开PyTorch官网，根据自己的系统配置进行选择，如下图所示。 上图是我电脑的选择。在刚刚建立的环境pt1中运行“Run this Command”对应的命令 1conda install pytorch torchvision cudatoolkit=9.2 -c pytorch -c defaults -c numba/label/dev 此过程较为耗时，等待全过程安装完毕。 测试安装完毕后，在pt1环境中打开Spyder，即，可在开始菜单单击“Spyder(pt1)”。 在Ipython中输入 123from __future__ import print_functionimport torch as tprint(t.cuda.is_available()) 若输出为True，则表明安装配置成功。 参考资料 PyTorch官网 NVIDIA驱动更新 PyTorch 1.0 中文文档&amp;教程]]></content>
      <categories>
        <category>程序语言</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>PyTorch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[写在开头]]></title>
    <url>%2F2019%2F09%2F25%2F%E5%86%99%E5%9C%A8%E5%BC%80%E5%A4%B4%2F</url>
    <content type="text"><![CDATA[“万事开头难。”历时3天，终于把个人博客捣鼓了出来。我采用的是GitHub+Hexo的搭建方式，NexT.Gemini主题，Gitalk评论插件。 在搭建过程中，我参考了网上很多的教程，此处特别感谢Zhechen’s Personal Website和韦阳的博客，让我的搭建过程少走了很多弯路。另外，官网Hexo和主题官网NexT也给了我很多帮助。 作为一名理工科的学生，我会通过博客，记录一些学习笔记和自认为有意义、有意思的事情。在帮助自己加深印象的同时，也希望能给同样寻求答案的人一些帮助。 我发现，网络上的博客质量参差不齐。希望我的博客能够始终保持高质量。既然写，就要写好、写清楚。]]></content>
      <categories>
        <category>随笔</category>
      </categories>
      <tags>
        <tag>总结</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2F2019%2F09%2F18%2Fhello-world%2F</url>
    <content type="text"><![CDATA[Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new "My New Post" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment]]></content>
  </entry>
</search>
